# Code Generation Standards

## Overview

All SQL and Python code generated by the analysis framework must follow strict documentation and commenting standards to ensure transparency, maintainability, and understanding.

---

## SQL Code Standards

### Required Elements

Every SQL query must include:

1. **Header Comment Block** - High-level explanation
2. **CTE Comments** - Explanation of each CTE's purpose
3. **Inline Comments** - Complex logic explanations
4. **Flow Explanation** - Step-by-step logic flow

### SQL Template

```sql
/*
 * QUERY PURPOSE: [Brief description of what this query does]
 * 
 * BUSINESS CONTEXT: [Why this analysis is needed]
 * 
 * LOGIC FLOW:
 * 1. [First step explanation]
 * 2. [Second step explanation]
 * 3. [Final step explanation]
 * 
 * ASSUMPTIONS:
 * - [Assumption 1]
 * - [Assumption 2]
 * 
 * OUTPUT: [What the result represents]
 */

-- Step 1: [Description of what this CTE does]
WITH step1_cte AS (
    SELECT 
        column1,  -- Explanation of why this column is selected
        column2   -- Explanation of this column's purpose
    FROM table1
    WHERE condition1  -- Explanation of filter logic
),

-- Step 2: [Description of what this CTE does]
step2_cte AS (
    SELECT 
        s1.column1,
        -- Complex calculation explanation
        CASE 
            WHEN condition THEN value1  -- Why this condition
            ELSE value2                 -- Fallback explanation
        END AS calculated_field
    FROM step1_cte s1
    JOIN table2 t2 ON s1.id = t2.id  -- Join purpose explanation
),

-- Final Step: [Description of final aggregation/transformation]
final_result AS (
    SELECT 
        segment_column,
        -- Aggregation explanation
        SUM(metric_column) AS total_metric,
        COUNT(*) AS record_count
    FROM step2_cte
    GROUP BY segment_column
)

-- Main query: [Final output explanation]
SELECT 
    segment_column,
    total_metric,
    record_count,
    -- Derived metric explanation
    ROUND(total_metric / record_count, 2) AS average_metric
FROM final_result
ORDER BY total_metric DESC;
```

### CTE Documentation Requirements

Each CTE must have:

1. **Purpose Statement**: What this CTE accomplishes
2. **Input Explanation**: What data it receives
3. **Transformation Logic**: What transformations it performs
4. **Output Description**: What data it produces

**Example:**

```sql
-- CTE: Calculate monthly recurring revenue by plan
-- Purpose: Aggregate subscription data to show MRR breakdown
-- Input: subscriptions and accounts tables
-- Logic: 
--   1. Join subscriptions to accounts to get plan information
--   2. Filter for active subscriptions only
--   3. Sum monthly_price grouped by plan
-- Output: Plan name and total MRR for each plan
WITH mrr_by_plan AS (
    SELECT 
        a.plan,                    -- Subscription plan tier
        SUM(s.monthly_price) AS mrr -- Total monthly recurring revenue
    FROM subscriptions s
    INNER JOIN accounts a ON s.org_id = a.id  -- Link subscription to account
    WHERE s.status = 'active'                  -- Only count active subscriptions
      AND a.plan IS NOT NULL                   -- Exclude accounts without plans
    GROUP BY a.plan
)
```

### Complex Logic Documentation

For complex calculations, always explain:

```sql
-- Calculate churn rate with cohort analysis
-- Logic:
--   1. Identify subscription start dates (cohorts)
--   2. Track cancellations per cohort
--   3. Calculate churn rate as canceled / total per cohort
--   4. Handle edge cases (nulls, zero divisions)
SELECT 
    cohort_month,
    total_subscriptions,
    canceled_subscriptions,
    -- Churn rate calculation with null handling
    CASE 
        WHEN total_subscriptions > 0 
        THEN ROUND(canceled_subscriptions::numeric / total_subscriptions * 100, 2)
        ELSE 0  -- Avoid division by zero
    END AS churn_rate_pct
FROM cohort_analysis;
```

---

## Python Code Standards

### Required Elements

Every Python function/class must include:

1. **Docstring** - Comprehensive function description
2. **Parameter Documentation** - Clear parameter explanations
3. **Return Value Documentation** - What the function returns
4. **Logic Flow Comments** - Step-by-step explanation
5. **Inline Comments** - Complex logic explanations

### Python Template

```python
def calculate_mrr_by_plan(
    subscriptions_df: pd.DataFrame,
    accounts_df: pd.DataFrame,
    status_filter: str = 'active'
) -> pd.DataFrame:
    """
    Calculate Monthly Recurring Revenue (MRR) grouped by subscription plan.
    
    BUSINESS CONTEXT:
    This function calculates the total MRR for each subscription plan tier,
    which is essential for understanding revenue distribution and plan performance.
    
    LOGIC FLOW:
    1. Filter subscriptions to active status only
    2. Merge subscriptions with accounts to get plan information
    3. Group by plan and sum monthly_price to get MRR per plan
    4. Sort by MRR descending for easy identification of top plans
    
    PARAMETERS:
        subscriptions_df (pd.DataFrame): DataFrame containing subscription data
            - Required columns: org_id, monthly_price, status
        accounts_df (pd.DataFrame): DataFrame containing account/plan data
            - Required columns: id, plan
        status_filter (str): Subscription status to include (default: 'active')
    
    RETURNS:
        pd.DataFrame: DataFrame with columns:
            - plan: Subscription plan name
            - mrr: Total monthly recurring revenue for the plan
            - subscription_count: Number of subscriptions in the plan
    
    ASSUMPTIONS:
        - subscriptions_df.org_id maps to accounts_df.id
        - monthly_price is in consistent currency units
        - status values are standardized (e.g., 'active', 'canceled')
    
    EXAMPLE:
        >>> subscriptions = pd.DataFrame({
        ...     'org_id': ['1', '2', '3'],
        ...     'monthly_price': [100, 200, 150],
        ...     'status': ['active', 'active', 'canceled']
        ... })
        >>> accounts = pd.DataFrame({
        ...     'id': ['1', '2', '3'],
        ...     'plan': ['Pro', 'Enterprise', 'Pro']
        ... })
        >>> result = calculate_mrr_by_plan(subscriptions, accounts)
        >>> print(result)
           plan  mrr  subscription_count
        0  Pro   100                   1
        1  Enterprise  200              1
    """
    # Step 1: Filter for active subscriptions only
    # This ensures we only count revenue from currently active subscriptions
    active_subscriptions = subscriptions_df[
        subscriptions_df['status'] == status_filter
    ].copy()
    
    # Step 2: Merge subscriptions with accounts to get plan information
    # Left join preserves all subscriptions even if account is missing
    merged = active_subscriptions.merge(
        accounts_df[['id', 'plan']],  # Only select needed columns
        left_on='org_id',
        right_on='id',
        how='left'  # Keep subscriptions even without matching account
    )
    
    # Step 3: Group by plan and calculate aggregations
    # This creates the MRR breakdown by plan tier
    mrr_by_plan = merged.groupby('plan', as_index=False).agg({
        'monthly_price': 'sum',      # Total MRR per plan
        'org_id': 'count'            # Number of subscriptions per plan
    }).rename(columns={
        'monthly_price': 'mrr',
        'org_id': 'subscription_count'
    })
    
    # Step 4: Sort by MRR descending
    # This puts the highest revenue plans first for easy analysis
    mrr_by_plan = mrr_by_plan.sort_values('mrr', ascending=False)
    
    return mrr_by_plan
```

### Class Documentation Standards

```python
class AnalysisFramework:
    """
    Main analysis framework for executing data analysis workflows.
    
    PURPOSE:
    This class orchestrates the complete analysis process, including:
    - Performance-aware query execution
    - Data validation
    - Step-by-step transparent analysis
    - Context management
    
    WORKFLOW:
    1. Initialize with schema context
    2. Connect to database
    3. Execute analysis steps with validation
    4. Generate transparent reports
    
    ATTRIBUTES:
        schema_context (Dict): Schema definitions from GitHub
        steps (List[AnalysisStep]): List of executed analysis steps
        connection: Database connection object
        table_metadata (Dict): Cached table metadata for performance
    
    EXAMPLE:
        >>> framework = AnalysisFramework(schema_context=my_schema)
        >>> framework.connect()
        >>> framework.add_step(description="...", query="...")
        >>> framework.close()
    """
    
    def add_step(self, description: str, query: str, **kwargs):
        """
        Add a new analysis step to the workflow.
        
        LOGIC FLOW:
        1. Extract table names from query for performance checking
        2. Check performance considerations (table sizes, filters)
        3. Execute query with timing
        4. Validate results if aggregation step
        5. Store step with all metadata
        
        This method ensures each step is:
        - Performance-optimized
        - Validated (if aggregation)
        - Fully documented
        - Transparently reported
        """
        # Implementation with comments...
```

---

## Code Generation Checklist

### Before Generating Code:

- [ ] Understand the business question/requirement
- [ ] Identify relevant tables and relationships
- [ ] Plan the logic flow
- [ ] Identify assumptions
- [ ] Consider edge cases

### During Code Generation:

- [ ] Add header comment block
- [ ] Document each CTE/function purpose
- [ ] Explain complex logic inline
- [ ] Document assumptions
- [ ] Add example usage (for functions)

### After Code Generation:

- [ ] Review for clarity
- [ ] Ensure all steps are explained
- [ ] Verify comments match code logic
- [ ] Check for missing edge case handling

---

## Examples

### Good SQL Example

```sql
/*
 * QUERY PURPOSE: Calculate Monthly Recurring Revenue (MRR) by subscription plan
 * 
 * BUSINESS CONTEXT: 
 * Need to understand revenue distribution across different plan tiers
 * to identify which plans drive the most revenue.
 * 
 * LOGIC FLOW:
 * 1. Filter subscriptions to active status only (exclude canceled/paused)
 * 2. Join with accounts to get plan information
 * 3. Aggregate monthly_price by plan to calculate total MRR per plan
 * 4. Calculate additional metrics (count, average) for context
 * 
 * ASSUMPTIONS:
 * - Only active subscriptions contribute to MRR
 * - monthly_price represents the recurring amount
 * - Accounts without plans are excluded from analysis
 * 
 * OUTPUT: Plan name, total MRR, subscription count, and average price
 */

-- Step 1: Get active subscriptions with account plan information
-- Purpose: Combine subscription data with plan details for aggregation
WITH active_subscriptions_with_plans AS (
    SELECT 
        s.id AS subscription_id,
        s.org_id,                    -- Link to account
        s.monthly_price,             -- Recurring revenue amount
        a.plan,                      -- Subscription plan tier
        s.status                     -- For validation
    FROM subscriptions s
    INNER JOIN accounts a ON s.org_id = a.id  -- Join to get plan info
    WHERE s.status = 'active'                   -- Only active subscriptions
      AND a.plan IS NOT NULL                    -- Exclude accounts without plans
),

-- Step 2: Aggregate MRR by plan
-- Purpose: Calculate total and average metrics per plan tier
mrr_by_plan AS (
    SELECT 
        plan,
        SUM(monthly_price) AS total_mrr,           -- Total MRR for plan
        COUNT(*) AS subscription_count,            -- Number of subscriptions
        AVG(monthly_price) AS avg_monthly_price    -- Average price per subscription
    FROM active_subscriptions_with_plans
    GROUP BY plan
)

-- Final output: MRR breakdown by plan with additional context
SELECT 
    plan,
    total_mrr,
    subscription_count,
    avg_monthly_price,
    -- Calculate MRR per subscription for comparison
    ROUND(total_mrr / subscription_count, 2) AS mrr_per_subscription
FROM mrr_by_plan
ORDER BY total_mrr DESC;  -- Highest revenue plans first
```

### Good Python Example

```python
def analyze_user_engagement(
    events_df: pd.DataFrame,
    users_df: pd.DataFrame,
    date_range: tuple = None
) -> pd.DataFrame:
    """
    Analyze user engagement metrics by calculating event activity per user.
    
    BUSINESS CONTEXT:
    Understanding user engagement helps identify active vs inactive users
    and measure product adoption. This analysis supports retention strategies.
    
    LOGIC FLOW:
    1. Filter events to date range (if specified)
    2. Merge events with user data to get user attributes
    3. Count events per user to measure activity
    4. Calculate engagement score based on event frequency
    5. Categorize users into engagement tiers
    
    PARAMETERS:
        events_df: Event data with columns [user_id, occurred_at, event_type]
        users_df: User data with columns [id, created_at, role]
        date_range: Optional (start_date, end_date) tuple for filtering
    
    RETURNS:
        DataFrame with user engagement metrics and categorization
    """
    # Step 1: Filter events by date range if provided
    # This allows analysis of specific time periods
    if date_range:
        start_date, end_date = date_range
        events_filtered = events_df[
            (events_df['occurred_at'] >= start_date) & 
            (events_df['occurred_at'] <= end_date)
        ].copy()
    else:
        events_filtered = events_df.copy()
    
    # Step 2: Count events per user
    # This measures activity level for each user
    user_activity = events_filtered.groupby('user_id', as_index=False).agg({
        'id': 'count',  # Total event count
        'event_type': 'nunique'  # Unique event types (engagement diversity)
    }).rename(columns={
        'id': 'total_events',
        'event_type': 'unique_event_types'
    })
    
    # Step 3: Merge with user data for context
    # Adds user attributes for segmentation analysis
    engagement_analysis = user_activity.merge(
        users_df[['id', 'created_at', 'role']],
        left_on='user_id',
        right_on='id',
        how='left'
    )
    
    # Step 4: Calculate engagement score
    # Weighted score: event count (70%) + diversity (30%)
    engagement_analysis['engagement_score'] = (
        engagement_analysis['total_events'] * 0.7 +
        engagement_analysis['unique_event_types'] * 10 * 0.3
    )
    
    # Step 5: Categorize users into engagement tiers
    # Helps identify power users, regular users, and inactive users
    engagement_analysis['engagement_tier'] = pd.cut(
        engagement_analysis['engagement_score'],
        bins=[0, 10, 50, 100, float('inf')],
        labels=['Low', 'Medium', 'High', 'Power User']
    )
    
    return engagement_analysis
```

---

## SQL Output After Execution

**CRITICAL: After executing any SQL query, you MUST output the formatted code**

### Required Output Format:

1. **Simple Explanation** - Plain language description (1-2 sentences)
2. **Formatted SQL Code** - Properly indented with all comments
3. **Results Summary** - Formatted results or summary

**Example:**
```
## Query Execution

**What this query does:**
This query calculates total MRR by adding up monthly prices from active subscriptions, grouped by plan type.

**SQL Code:**
```sql
[Formatted SQL with all comments preserved]
```

**Results:**
[Results summary]
```

**See `SQL_OUTPUT_STANDARDS.md` for complete requirements**

## Enforcement

These standards must be followed for:

- ✅ All SQL queries generated by the framework
- ✅ All Python functions in the framework
- ✅ All analysis steps added via `add_step()`
- ✅ All custom analysis functions
- ✅ All code examples in documentation
- ✅ **All SQL output after execution** (formatted with explanation)

**No exceptions** - code without proper documentation will be rejected.

